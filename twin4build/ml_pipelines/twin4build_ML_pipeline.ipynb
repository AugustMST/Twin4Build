{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_pipe_space_model_batches import insert_data\n",
    "dataset_df  = insert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from ml_pipe_data_collection import DataCollection\n",
    "\n",
    "room_id  =\"room_1\"\n",
    "\n",
    "def preprocessing_function(dataset,room_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Args: Require dataframe as input,\n",
    "    Room id to create folder for that room and save data for specific room.\n",
    "\n",
    "    return:Numpy file at specific folder\n",
    "\n",
    "      \"\"\"\n",
    "    data_collection = DataCollection(room_id, dataset, nan_interpolation_gap_limit=36, n_sequence=144)\n",
    "    data_collection.prepare_for_data_batches()\n",
    "\n",
    "    # this folder structure is created for jupyter notebbook\n",
    "    save_folder = os.path.join(room_id, \"space_model_dataset\")\n",
    "    if not os.path.exists(save_folder):\n",
    "    # create the directory\n",
    "        os.makedirs(save_folder)\n",
    "    data_collection.create_data_batches(save_folder=save_folder)\n",
    "\n",
    "    save_folder = os.path.join(room_id, \"space_models\", \"BMS_data\")\n",
    "    if not os.path.exists(save_folder):\n",
    "    # create the directory\n",
    "        os.makedirs(save_folder)\n",
    "    data_collection.save_building_data_collection_dict(save_folder=save_folder)\n",
    "\n",
    "preprocessing_function(dataset_df,room_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import Transform\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ModelValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_data_validation.utils import slicing_util\n",
    "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fed956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(\"room_1\\\\space_model_dataset\\\\room_1_training.npz\")\n",
    "\n",
    "x = data[data.files[0]]\n",
    "y = data[data.files[1]]\n",
    "\n",
    "x = x[:,:-1]\n",
    "y= y[:,1:]-y[:,:-1]\n",
    "\n",
    "# Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "assert x.shape[0] == y.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a196996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistics from the dataset\n",
    "stats = tfdv.generate_statistics_from_tfrecord(dataset)\n",
    "tfdv.visualize_statistics(stats)\n",
    "\n",
    "# Define the schema for the dataset\n",
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "\n",
    "# Print the schema and the anomalies\n",
    "print(\"Schema:\")\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e97a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_stats = tfdv.generate_statistics_from_dataframe(df)\n",
    "tfdv.visualize_statistics(Data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49662862",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=Data_stats)\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c5106-b06c-4b1f-a71b-eb94f71b6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output locations for the pipeline\n",
    "input_data = \"'temp_file_room_1.csv'\"\n",
    "output_data = \"./\"\n",
    "\n",
    "# Define the pipeline\n",
    "example_gen = CsvExampleGen(input_base=input_data)\n",
    "statistics_gen = StatisticsGen(input_data=example_gen.outputs['examples'])\n",
    "schema_gen = SchemaGen(input_data=statistics_gen.outputs['output'])\n",
    "example_validator = ExampleValidator(stats=statistics_gen.outputs['output'], schema=schema_gen.outputs['schema'])\n",
    "transform = Transform(input_data=example_gen.outputs['examples'], schema=schema_gen.outputs['schema'])\n",
    "trainer = Trainer(input_data=transform.outputs['transformed_examples'], schema=schema_gen.outputs['schema'])\n",
    "evaluator = Evaluator(input_data=transform.outputs['transformed_examples'], schema=schema_gen.outputs['schema'], model=trainer.outputs['model'])\n",
    "model_validator = ModelValidator(model=trainer.outputs['model'], examples=example_gen.outputs['examples'], schema=schema_gen.outputs['schema'])\n",
    "pusher = Pusher(input_data=trainer.outputs['model'])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_name = 'my_pipeline'\n",
    "pipeline_root = output_data\n",
    "pipeline_args = {}\n",
    "pipeline_args['pipeline_name'] = pipeline_name\n",
    "pipeline_args['pipeline_root'] = pipeline_root\n",
    "pipeline_args['input_data'] = input_data\n",
    "pipeline_args['output_data'] = output_data\n",
    "pipeline_args['components'] = [example_gen, statistics_gen,schema_gen, example_validator, transform, trainer, evaluator, model_validator, pusher]\n",
    "\n",
    "# Create and run the pipeline\n",
    "p = pipeline.Pipeline(**pipeline_args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
